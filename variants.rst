Chapter 4:  Alternative Algorithms
==================================

This chapter describes many variations of the original TCP congestion
control algorithm have evolved over the years. One common theme is
that there is a rich design space (and range of use cases), with each
variant focused on a different combination of factors. We start by
sketching the dimensions of the design space as a way of framing these
algorithms.

*[The following is cut-and-pasted: the order still needs attenion, and
the treatment is uneven.]*

4.1  Framing
------------

*[Among other things, this is a good place to contrast congestion
control with congestion avoidance.]*

4.2 TCP Vegas
-------------

The mechanism we are going to describe looks at changes in the
throughput rate or, more specifically, changes in the sending
rate. However, it differs from the previous algorithm in the way it
calculates throughput, and instead of looking for a change in the
slope of the throughput it compares the measured throughput rate with
an expected throughput rate. The algorithm, TCP Vegas, is not widely
deployed in the Internet today, but the strategy it uses has been
adopted by other implementations that are now being deployed.

The intuition behind the Vegas algorithm can be seen in the trace of
standard TCP given in :numref:`Figure %s <fig-trace3>`. The top graph
shown in :numref:`Figure %s <fig-trace3>` traces the connection’s
congestion window; it shows the same information as the traces given
earlier in this section.  The middle and bottom graphs depict new
information: The middle graph shows the average sending rate as
measured at the source, and the bottom graph shows the average queue
length as measured at the bottleneck router. All three graphs are
synchronized in time. In the period between 4.5 and 6.0 seconds
(shaded region), the congestion window increases (top graph). We
expect the observed throughput to also increase, but instead it stays
flat (middle graph). This is because the throughput cannot increase
beyond the available bandwidth. Beyond this point, any increase in the
window size only results in packets taking up buffer space at the
bottleneck router (bottom graph).

.. _fig-trace3:
.. figure:: figures/f06-18-9780123850591.png
   :width: 600px
   :align: center

   Congestion window versus observed throughput rate (the
   three graphs are synchronized). Top, congestion window; middle,
   observed throughput; bottom, buffer space taken up at the
   router. Colored line = `CongestionWindow`; solid bullet = timeout;
   hash marks = time when each packet is transmitted; vertical bars =
   time when a packet that was eventually retransmitted was first
   transmitted.

A useful metaphor that describes the phenomenon illustrated in
:numref:`Figure %s <fig-trace3>` is driving on ice. The speedometer
(congestion window) may say that you are going 30 miles an hour, but
by looking out the car window and seeing people pass you on foot
(measured sending rate) you know that you are going no more than 5
miles an hour. The extra energy is being absorbed by the car’s tires
(router buffers).

TCP Vegas uses this idea to measure and control the amount of extra data
this connection has in transit, where by “extra data” we mean data that
the source would not have transmitted had it been trying to match
exactly the available bandwidth of the network. The goal of TCP Vegas is
to maintain the “right” amount of extra data in the network. Obviously,
if a source is sending too much extra data, it will cause long delays
and possibly lead to congestion. Less obviously, if a connection is
sending too little extra data, it cannot respond rapidly enough to
transient increases in the available network bandwidth. TCP Vegas’s
congestion-avoidance actions are based on changes in the estimated
amount of extra data in the network, not only on dropped packets. We now
describe the algorithm in detail.

First, define a given flow’s ``BaseRTT`` to be the RTT of a packet when
the flow is not congested. In practice, TCP Vegas sets ``BaseRTT`` to
the minimum of all measured round-trip times; it is commonly the RTT of
the first packet sent by the connection, before the router queues
increase due to traffic generated by this flow. If we assume that we are
not overflowing the connection, then the expected throughput is given by

::

   ExpectedRate = CongestionWindow / BaseRTT

where ``CongestionWindow`` is the TCP congestion window, which we
assume (for the purpose of this discussion) to be equal to the number
of bytes in transit.

Second, TCP Vegas calculates the current sending rate, ``ActualRate``.
This is done by recording the sending time for a distinguished packet,
recording how many bytes are transmitted between the time that packet
is sent and when its acknowledgment is received, computing the sample
RTT for the distinguished packet when its acknowledgment arrives, and
dividing the number of bytes transmitted by the sample RTT. This
calculation is done once per round-trip time.

Third, TCP Vegas compares ``ActualRate`` to ``ExpectedRate`` and
adjusts the window accordingly. We let ``Diff = ExpectedRate -
ActualRate``.  Note that ``Diff`` is positive or 0 by definition,
since ``ActualRate >ExpectedRate`` implies that we need to change
``BaseRTT`` to the latest sampled RTT. We also define two thresholds,
*α < β*, roughly corresponding to having too little and too much extra
data in the network, respectively. When ``Diff`` < *α*, TCP Vegas
increases the congestion window linearly during the next RTT, and when
``Diff`` > *β*, TCP Vegas decreases the congestion window linearly
during the next RTT.  TCP Vegas leaves the congestion window unchanged
when *α* < ``Diff`` < *β*.

Intuitively, we can see that the farther away the actual throughput
gets from the expected throughput, the more congestion there is in the
network, which implies that the sending rate should be reduced. The
*β* threshold triggers this decrease. On the other hand, when the
actual throughput rate gets too close to the expected throughput, the
connection is in danger of not utilizing the available bandwidth. The
*α* threshold triggers this increase. The overall goal is to keep
between\ *α* and *β* extra bytes in the network.

.. _fig-vegas:
.. figure:: figures/f06-19-9780123850591.png
   :width: 600px
   :align: center

   Trace of TCP Vegas congestion-avoidance mechanism.
   Top, congestion window; bottom, expected (colored line) and actual
   (black line) throughput. The shaded area is the region between the
   *α* and *β* thresholds.

:numref:`Figure %s <fig-vegas>` traces the TCP Vegas
congestion-avoidance algorithm. The top graph traces the congestion
window, showing the same information as the other traces given
throughout this chapter. The bottom graph traces the expected and
actual throughput rates that govern how the congestion window is
set. It is this bottom graph that best illustrates how the algorithm
works. The colored line tracks the ``ExpectedRate``, while the black
line tracks the ``ActualRate``. The wide shaded strip gives the region
between the *α* and *β* thresholds; the top of the shaded strip is
*α* KBps away from ``ExpectedRate``, and the bottom of the shaded
strip is *β* KBps away from ``ExpectedRate``.  The goal is to keep the
``ActualRate`` between these two thresholds, within the shaded
region. Whenever ``ActualRate`` falls below the shaded region (i.e.,
gets too far from ``ExpectedRate``), TCP Vegas decreases the
congestion window because it fears that too many packets are being
buffered in the network. Likewise, whenever ``ActualRate`` goes above
the shaded region (i.e., gets too close to the ``ExpectedRate``), TCP
Vegas increases the congestion window because it fears that it is
underutilizing the network.

Because the algorithm, as just presented, compares the difference
between the actual and expected throughput rates to the *α* and *β*
thresholds, these two thresholds are defined in terms of KBps. However,
it is perhaps more accurate to think in terms of how many extra
*buffers* the connection is occupying in the network. For example, on a
connection with a ``BaseRTT`` of 100 ms and a packet size of 1 KB, if
*α* = 30 KBps and *β* = 60 KBps, then we can think of *α* as specifying
that the connection needs to be occupying at least 3 extra buffers in
the network and *β* as specifying that the connection should occupy no
more than 6 extra buffers in the network. In practice, a setting of *α*
to 1 buffer and *β* to 3 buffers works well.

Finally, you will notice that TCP Vegas decreases the congestion window
linearly, seemingly in conflict with the rule that multiplicative
decrease is needed to ensure stability. The explanation is that TCP
Vegas does use multiplicative decrease when a timeout occurs; the linear
decrease just described is an *early* decrease in the congestion window
that should happen before congestion occurs and packets start being
dropped.

4.3 TCP BBR
-----------

BBR (Bottleneck Bandwidth and RTT) is a new TCP congestion control
algorithm developed by researchers at Google. Like Vegas, BBR is delay
based, which means it tries to detect buffer growth so as to avoid
congestion and packet loss. Both BBR and Vegas use the minimum RTT and
maximum RTT, as calculated over some time interval, as their main
control signals.

BBR also introduces new mechanisms to improve performance, including
packet pacing, bandwidth probing, and RTT probing. Packet pacing spaces
the packets based on the estimate of the available bandwidth. This
eliminates bursts and unnecessary queueing, which results in a better
feedback signal. BBR also periodically increases its rate, thereby
probing the available bandwidth. Similarly, BBR periodically decreases
its rate, thereby probing for a new minimum RTT. The RTT probing
mechanism attempts to be self-synchronizing, which is to say, when there
are multiple BBR flows, their respective RTT probes happen at the same
time. This gives a more accurate view of the actual uncongested path
RTT, which solves one of the major issues with delay-based congestion
control mechanisms: having accurate knowledge of the uncongested path
RTT.

BBR is actively being worked on and rapidly evolving. One major focus is
fairness. For example, some experiments show CUBIC flows get 100× less
bandwidth when competing with BBR flows, and other experiments show that
unfairness among BBR flows is even possible. Another major focus is
avoiding high retransmission rates, where in some cases as many as 10%
of packets are retransmitted.

4.4 DCTCP
---------------

We conclude with an example of a situation where a variant of the TCP
congestion control algorithm has been designed to work in concert with
ECN: in cloud datacenters. The combination is called DCTCP, which stands
for *Data Center TCP*. The situation is unique in that a datacenter is
self-contained, and so it is possible to deploy a tailor-made version of
TCP that does not need to worry about treating other TCP flows fairly.
Datacenters are also unique in that they are built using low-cost
white-box switches, and because there is no need to worry about long-fat
pipes spanning a continent, the switches are typically provisioned
without an excess of buffers.

The idea is straightforward. DCTCP adapts ECN by estimating the fraction
of bytes that encounter congestion rather than simply detecting that
some congestion is about to occur. At the end hosts, DCTCP then scales
the congestion window based on this estimate. The standard TCP algorithm
still kicks in should a packet actually be lost. The approach is
designed to achieve high-burst tolerance, low latency, and high
throughput with shallow-buffered switches.

The key challenge DCTCP faces is to estimate the fraction of bytes
encountering congestion. Each switch is simple. If a packet arrives and
the switch sees the queue length (K) is above some threshold; e.g.,

.. centered:: K > (RTT × C)/7

where C is the link rate in packets per second, then the switch sets the
CE bit in the IP header. The complexity of RED is not required.

The receiver then maintains a boolean variable for every flow, which
we’ll denote ``SeenCE``, and implements the following state machine in
response to every received packet:

-  If the CE bit is set and ``SeenCE=False``, set ``SeenCE`` to True and
   send an immediate ACK.

-  If the CE bit is not set and ``SeenCE=True``, set ``SeenCE`` to False
   and send an immediate ACK.

-  Otherwise, ignore the CE bit.

The non-obvious consequence of the “otherwise” case is that the receiver
continues to send delayed ACKs once every *n* packets, whether or not
the CE bit is set. This has proven important to maintaining high
performance.

Finally, the sender computes the fraction of bytes that encountered
congestion during the previous observation window (usually chosen to be
approximately the RTT), as the ratio of the total bytes transmitted and
the bytes acknowledged with the ECE flag set. DCTCP grows the congestion
window in exactly the same way as the standard algorithm, but it reduces
the window in proportion to how many bytes encountered congestion during
the last observation window.

4.5 TCP CUBIC 
--------------

A variant of the standard TCP algorithm, called CUBIC, is the default
congestion control algorithm distributed with Linux. CUBIC’s primary
goal is to support networks with large delay × bandwidth products,
which are sometimes called *long-fat networks*. Such networks suffer
from the original TCP algorithm requiring too many round-trips to
reach the available capacity of the end-to-end path. CUBIC does this
by being more aggressive in how it increases the window size, but of
course the trick is to be more aggressive without being so aggressive
as to adversely affect other flows.

One important aspect of CUBIC’s approach is to adjust its congestion 
window at regular intervals, based on the amount of time that has 
elapsed since the last congestion event (e.g., the arrival of a 
duplicate ACK), rather than only when ACKs arrive (the latter being a 
function of RTT). This allows CUBIC to behave fairly when competing with 
short-RTT flows, which will have ACKs arriving more frequently. 

.. _fig-cubic:
.. figure:: figures/Slide1.png 
   :width: 500px 
   :align: center 

   Generic cubic function illustrsting the change in the congestion 
   window as a function of time. 

The second important aspect of CUBIC is its use of a cubic function to 
adjust the congestion window. The basic idea is easiest to understand 
by looking at the general shape of a cubic function, which has three 
phases: slowing growth, flatten plateau, increasing growth. A generic 
example is shown in :numref:`Figure %s <fig-cubic>`, which we have 
annotated with one extra piece of information: the maximum congestion 
window size achieved just before the last congestion event as a target 
(denoted :math:`W_{max}`). The idea is to start fast but slow the 
growth rate as you get close to :math:`W_{max}`, be cautious and have 
near-zero growth when close to :math:`W_{max}`, and then increase the 
growth rate as you move away from :math:`W_{max}`. The latter phase is 
essentially probing for a new achievable :math:`W_{max}`. 

Specifically, CUBIC computes the congestion window as a function of time 
(t) since the last congestion event 

.. math::

   \mathsf{CWND(t)} = \mathsf{C} \times \mathsf{(t-K)}^{3} + \mathsf{W}_{max}

where 

.. math::

   \mathsf{K} =  \sqrt[3]{\mathsf{W}_{max} \times (1 - \beta{})/\mathsf{C}}

C is a scaling constant and :math:`\beta` is the multiplicative 
decrease factor.  CUBIC sets the latter to 0.7 rather than the 0.5 
that standard TCP uses. Looking back at :numref:`Figure %s 
<fig-cubic>`, CUBIC is often described as shifting between a concave 
function to being convex (whereas standard TCP’s additive function is 
only convex). 

